{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "77e05df3218e009eafa91b1afffbb5cdd6535a41",
    "collapsed": true
   },
   "source": [
    "# Simple Unet CV Training\n",
    "\n",
    "1. Basic Unet model with resnet blocks.\n",
    "2. starting layer 128*128, 16 channels\n",
    "3. Middle layer 4*4, 512 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from random import randint\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from skimage.transform import resize\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import load_model, save_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\n",
    "from keras.layers import Conv2D, Concatenate, MaxPooling2D\n",
    "from keras.layers import UpSampling2D, Dropout, BatchNormalization\n",
    "from keras.losses import binary_crossentropy\n",
    "from tqdm import tqdm_notebook\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "962c2c6775b5fcf605df8e7c59cbcabe6ba9ceaa"
   },
   "source": [
    "# Params and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e54e151245d665e42bb95d9cf2e1a33cb9440e48"
   },
   "outputs": [],
   "source": [
    "img_size_ori = 101\n",
    "img_size_target = 128\n",
    "\n",
    "def upsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n",
    "    #res[:img_size_ori, :img_size_ori] = img\n",
    "    #return res\n",
    "    \n",
    "def downsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n",
    "    #return img[:img_size_ori, :img_size_ori]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c4bf5e157d253743b97820cb7fc5738235969f1"
   },
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close approximation of competition metric\n",
    "\n",
    "import tensorflow as tf\n",
    "#Score the model and do a threshold optimization by the best IoU.\n",
    "\n",
    "# src: https://www.kaggle.com/aglotero/another-iou-metric\n",
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = y_true_in\n",
    "    y_pred = y_pred_in\n",
    "\n",
    "\n",
    "    true_objects = 2\n",
    "    pred_objects = 2\n",
    "\n",
    "    # Jiaxin fin that if all zeros, then, the background is treated as object\n",
    "    temp1 = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=([0,0.5,1], [0,0.5, 1]))\n",
    "#     temp1 = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))\n",
    "    #print(temp1)\n",
    "    intersection = temp1[0]\n",
    "    #print(\"temp2 = \",temp1[1])\n",
    "    #print(intersection.shape)\n",
    "   # print(intersection)\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    #print(np.histogram(labels, bins = true_objects))\n",
    "    area_true = np.histogram(labels,bins=[0,0.5,1])[0]\n",
    "    #print(\"area_true = \",area_true)\n",
    "    area_pred = np.histogram(y_pred, bins=[0,0.5,1])[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    intersection[intersection == 0] = 1e-9\n",
    "    \n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    y_pred_in = y_pred_in > 0.5 # added by sgx 20180728\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "        metric.append(value)\n",
    "    #print(\"metric = \",metric)\n",
    "    return np.mean(metric)\n",
    "\n",
    "def my_iou_metric(label, pred):\n",
    "    metric_value = tf.py_func(iou_metric_batch, [label, pred], tf.float64)\n",
    "    return metric_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b18c1f50cefd7504eae7e7b9605be3814c7cad6d"
   },
   "outputs": [],
   "source": [
    "train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "86620c6a070571895f4f36ec050a25803915ed74"
   },
   "outputs": [],
   "source": [
    "train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1137f0a009f10b5f69e4dade5f689e744e9ce1d6"
   },
   "source": [
    "# Calculating the salt coverage and salt coverage classes\n",
    "Counting the number of salt pixels in the masks and dividing them by the image size. Also create 11 coverage classes, -0.1 having no salt at all to 1.0 being salt only.\n",
    "Plotting the distribution of coverages and coverage classes, and the class against the raw coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "18d2aa182a44c65a87c75f41047c653a79bc1c3f"
   },
   "outputs": [],
   "source": [
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b13d1ecc7004832e8e042d034922796263054b7"
   },
   "outputs": [],
   "source": [
    "def cov_to_class(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "        \n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5e66ff4809ea2f9a679b7ddbda5028dc324137a"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "sns.distplot(train_df.coverage, kde=False, ax=axs[0])\n",
    "sns.distplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\n",
    "plt.suptitle(\"Salt coverage\")\n",
    "axs[0].set_xlabel(\"Coverage\")\n",
    "axs[1].set_xlabel(\"Coverage class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0dd39993eb2c7e77e5ce2d3388ea8ff1d581a670"
   },
   "outputs": [],
   "source": [
    "plt.scatter(train_df.coverage, train_df.coverage_class)\n",
    "plt.xlabel(\"Coverage\")\n",
    "plt.ylabel(\"Coverage class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2391c568019151b098a002937516bb77a506f403"
   },
   "source": [
    "# Plotting the depth distributions\n",
    "Separatelty plotting the depth distributions for the training and the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ae7b7011b7de3caed58f9ca3939df15ffa319ad"
   },
   "outputs": [],
   "source": [
    "sns.distplot(train_df.z, label=\"Train\")\n",
    "sns.distplot(test_df.z, label=\"Test\")\n",
    "plt.legend()\n",
    "plt.title(\"Depth distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14835b3e0eafd3a1c0e3a1f18a2e7979e75d3fa3"
   },
   "source": [
    "# Show some example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a6bc85ee458f72c0917edf77895d5abc5eaf3ee",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_images = 60\n",
    "grid_width = 15\n",
    "grid_height = int(max_images / grid_width)\n",
    "fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n",
    "for i, idx in enumerate(train_df.index[:max_images]):\n",
    "    img = train_df.loc[idx].images\n",
    "    mask = train_df.loc[idx].masks\n",
    "    ax = axs[int(i / grid_width), i % grid_width]\n",
    "    ax.imshow(img, cmap=\"Greys\")\n",
    "    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n",
    "    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n",
    "    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n",
    "    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "plt.suptitle(\"Green: salt. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "00655e32f93f96ebd90dbe94e35ee052f52217cd"
   },
   "source": [
    "# Create train/validation split stratified by salt coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2f6ab5144292789d0c1992d51688b7fa7eca454"
   },
   "outputs": [],
   "source": [
    "# 5 fold cross validation , stratified based on salt coverage\n",
    "\n",
    "ids_train = []\n",
    "ids_valid = []\n",
    "skf = StratifiedKFold(n_splits=5, random_state=1338)\n",
    "i = 0\n",
    "for indices in skf.split(X=train_df.index.values,y=train_df.coverage_class):\n",
    "    if i==4:\n",
    "        ids_train = indices[0]\n",
    "        ids_valid = indices[1]\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "x_train = np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)[ids_train]\n",
    "x_valid = np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)[ids_valid]\n",
    "y_train = np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)[ids_train]\n",
    "y_valid = np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)[ids_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8696336e8d53c7943c4479eb556503fbcd4cd7ee"
   },
   "outputs": [],
   "source": [
    "print(x_train.shape, x_valid.shape)\n",
    "print(y_train.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "63ac58ab47921b4e4f54102e2c8b85fa318225f1"
   },
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fafc961e3300b54542a2c0db9654ab3f8b47c822"
   },
   "outputs": [],
   "source": [
    "def iou_coef(y_true, y_pred, smooth=1):\n",
    "    print(y_true.shape, y_pred.shape)\n",
    "    iou = 0.\n",
    "    for i in range(16):\n",
    "        intersection = K.sum(K.abs(y_true[i,:,:,:] * y_pred[i,:,:,:]))\n",
    "        print(intersection)\n",
    "        union = K.sum(y_true[i,:,:,:]) + K.sum(y_pred[i,:,:,:]) - intersection\n",
    "        iou = (intersection + smooth) / ( union + smooth)\n",
    "    return iou/16\n",
    "\n",
    "def iou_coef_loss(y_true, y_pred):\n",
    "    return -iou_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1aa78bd7c607e1f0e0235e4b2f82056c0361dac5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.layers import Add\n",
    "# Build U-Net model\n",
    "input_img = Input((img_size_target, img_size_target, 1), name='img')\n",
    "\n",
    "c1 = Conv2D(16, (3, 3), activation='relu', padding='same') (input_img)\n",
    "c1 = BatchNormalization()(c1)\n",
    "c1 = Conv2D(16, (3, 3), activation='relu', padding='same') (c1)\n",
    "c1 = BatchNormalization()(c1)\n",
    "d1 = Conv2D(16, (1, 1), activation='relu', padding='same') (input_img)\n",
    "c1 = Add()([c1, d1])\n",
    "c1 = BatchNormalization()(c1)\n",
    "p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "c2 = Conv2D(32, (3, 3), activation='relu', padding='same') (p1)\n",
    "c2 = BatchNormalization()(c2)\n",
    "c2 = Conv2D(32, (3, 3), activation='relu', padding='same') (c2)\n",
    "c2 = BatchNormalization()(c2)\n",
    "d2 = Conv2D(32, (1, 1), activation='relu', padding='same') (p1)\n",
    "c2 = Add()([c2, d2])\n",
    "c2 = BatchNormalization()(c2)\n",
    "p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "c3 = Conv2D(64, (3, 3), activation='relu', padding='same') (p2)\n",
    "c3 = BatchNormalization()(c3)\n",
    "c3 = Conv2D(64, (3, 3), activation='relu', padding='same') (c3)\n",
    "c3 = BatchNormalization()(c3)\n",
    "d3 = Conv2D(64, (1, 1), activation='relu', padding='same') (p2)\n",
    "c3 = Add()([c3, d3])\n",
    "c3 = BatchNormalization()(c3)\n",
    "p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "c4 = Conv2D(128, (3, 3), activation='relu', padding='same') (p3)\n",
    "c4 = BatchNormalization()(c4)\n",
    "c4 = Conv2D(128, (3, 3), activation='relu', padding='same') (c4)\n",
    "c4 = BatchNormalization()(c4)\n",
    "d4 = Conv2D(128, (1, 1), activation='relu', padding='same') (p3)\n",
    "c4 = Add()([c4, d4])\n",
    "c4 = BatchNormalization()(c4)\n",
    "p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "#p4 = Dropout(0.5)(p4)\n",
    "\n",
    "c5 = Conv2D(256, (3, 3), activation='relu', padding='same') (p4)\n",
    "c5 = BatchNormalization()(c5)\n",
    "c5 = Conv2D(256, (3, 3), activation='relu', padding='same') (c5)\n",
    "c5 = BatchNormalization()(c5)\n",
    "d5 = Conv2D(256, (1, 1), activation='relu', padding='same') (p4)\n",
    "c5 = Add()([c5, d5])\n",
    "c5 = BatchNormalization()(c5)\n",
    "p5 = MaxPooling2D(pool_size=(2, 2)) (c5)\n",
    "\n",
    "c6 = Conv2D(512, (3, 3), activation='relu', padding='same') (p5)\n",
    "c6 = BatchNormalization()(c6)\n",
    "c6 = Dropout(0.5)(c6)\n",
    "c6 = Conv2D(512, (3, 3), activation='relu', padding='same') (c6)\n",
    "c6 = BatchNormalization()(c6)\n",
    "\n",
    "u7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "u7 = concatenate([u7, c5])\n",
    "c7 = Conv2D(256, (3, 3), activation='relu', padding='same') (u7)\n",
    "c7 = BatchNormalization()(c7)\n",
    "c7 = Conv2D(256, (3, 3), activation='relu', padding='same') (c7)\n",
    "c7 = BatchNormalization()(c7)\n",
    "d7 = Conv2D(256, (1, 1), activation='relu', padding='same') (u7)\n",
    "c7 = Add()([c7, d7])\n",
    "c7 = BatchNormalization()(c7)\n",
    "\n",
    "u8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "u8 = concatenate([u8, c4])\n",
    "#u7 = Dropout(0.5)(u7)\n",
    "c8 = Conv2D(128, (3, 3), activation='relu', padding='same') (u8)\n",
    "c8 = BatchNormalization()(c8)\n",
    "c8 = Conv2D(128, (3, 3), activation='relu', padding='same') (c8)\n",
    "c8 = BatchNormalization()(c8)\n",
    "d8 = Conv2D(128, (1, 1), activation='relu', padding='same') (u8)\n",
    "c8 = Add()([c8, d8])\n",
    "c8 = BatchNormalization()(c8)\n",
    "\n",
    "u9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "u9 = concatenate([u9, c3])\n",
    "#u8 = Dropout(0.5)(u8)\n",
    "c9 = Conv2D(64, (3, 3), activation='relu', padding='same') (u9)\n",
    "c9 = BatchNormalization()(c9)\n",
    "c9 = Conv2D(64, (3, 3), activation='relu', padding='same') (c9)\n",
    "c9 = BatchNormalization()(c9)\n",
    "d9 = Conv2D(64, (1, 1), activation='relu', padding='same') (u9)\n",
    "c9 = Add()([c9, d9])\n",
    "c9 = BatchNormalization()(c9)\n",
    "\n",
    "u10 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c9)\n",
    "u10 = concatenate([u10, c2])\n",
    "#u8 = Dropout(0.5)(u8)\n",
    "c10 = Conv2D(32, (3, 3), activation='relu', padding='same') (u10)\n",
    "c10 = BatchNormalization()(c10)\n",
    "c10 = Conv2D(32, (3, 3), activation='relu', padding='same') (c10)\n",
    "c10 = BatchNormalization()(c10)\n",
    "d10 = Conv2D(32, (1, 1), activation='relu', padding='same') (u10)\n",
    "c10 = Add()([c10, d10])\n",
    "c10 = BatchNormalization()(c10)\n",
    "\n",
    "u11 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c10)\n",
    "u11 = concatenate([u11, c1], axis=3)\n",
    "#u9 = Dropout(0.6)(u9)\n",
    "c11 = Conv2D(16, (3, 3), activation='relu', padding='same') (u11)\n",
    "c11 = BatchNormalization()(c11)\n",
    "c11 = Conv2D(16, (2, 2), activation='relu', padding='same') (c11)\n",
    "c11 = BatchNormalization()(c11)\n",
    "d11 = Conv2D(16, (1, 1), activation='relu', padding='same') (u11)\n",
    "c11 = Add()([c11, d11])\n",
    "c11 = BatchNormalization()(c11)\n",
    "\n",
    "outputs = Conv2D(1, (1, 1), activation='sigmoid') (c11)\n",
    "opt = Adam(lr=0.0012)\n",
    "model = Model(inputs=[input_img], outputs=[outputs])\n",
    "model.compile(optimizer=opt, loss=bce_dice_loss, metrics=[my_iou_metric]) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c007157c2fd3d7dadcaeee2a6376351852d1e565"
   },
   "source": [
    "# Data augmentation\n",
    "\n",
    "1. Horizontal flips\n",
    "2. corner based random crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b01f71a37fb2ea9f45e42fcce10869ebbb9664b"
   },
   "outputs": [],
   "source": [
    "from random import randint, sample\n",
    "def cropsample(img):\n",
    "    c = randint(0,4)\n",
    "    if c==0:\n",
    "        return resize(img[:104,:104], (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    elif c==1:\n",
    "        return resize(img[24:,:104], (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    elif c==2:\n",
    "        return resize(img[:104,24:], (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    else:\n",
    "        return resize(img[24:,24:], (img_size_target, img_size_target), mode='constant', preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "88b3f57eac3ec3719b401730dc6d8d2d89d09ccc"
   },
   "outputs": [],
   "source": [
    "x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n",
    "y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)\n",
    "print(len(x_train), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "472198e03e965b8b4356f85d50b54c167e3b670c"
   },
   "outputs": [],
   "source": [
    "random.seed(1007)\n",
    "x_train = np.append(x_train, [cropsample(x) for x in tqdm_notebook(x_train)], axis=0)\n",
    "random.seed(1007)\n",
    "y_train = np.append(y_train, [cropsample(x) for x in tqdm_notebook(y_train)], axis=0)\n",
    "print(len(x_train), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7040f72549212dd4f71c13dfbd8bf013481ea369"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 10, figsize=(15,3))\n",
    "for i in range(10):\n",
    "    axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n",
    "    axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n",
    "    axs[1][i].imshow(x_train[int(len(x_train)/2 + i)].squeeze(), cmap=\"Greys\")\n",
    "    axs[1][i].imshow(y_train[int(len(y_train)/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\n",
    "fig.suptitle(\"Top row: original images, bottom row: augmented images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f5a6b1abaa4681cba3b608bc5f33cf260370d82a"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fba3d8b3838e7e68d56963e2306c609125859994"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.0012\n",
    "    drop = 0.5\n",
    "    epochs_drop = 8.0\n",
    "    lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop))\n",
    "    print(\"lrate reduced to\", lrate)\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f1773642758da7b4480e0e48c045bd01ea3684ae",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=20, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(\"./keras4.model\", save_best_only=True, verbose=1)\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "epochs = 60\n",
    "batch_size = 16\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=[x_valid, y_valid], \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint, lrate],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42e9ef3c4e0a2bb2539e5e51740ba6bfc092d37c"
   },
   "outputs": [],
   "source": [
    "fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "ax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax_acc.plot(history.epoch, history.history[\"my_iou_metric\"], label=\"Train accuracy\")\n",
    "ax_acc.plot(history.epoch, history.history[\"val_my_iou_metric\"], label=\"Validation accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c824f6bca47f051500966c433ce7fb5a9528f6d7"
   },
   "outputs": [],
   "source": [
    "save_model(model, \"./endkeras4.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd6ce9b4d5fc80a2502a43e80299d628fb5ffc42"
   },
   "outputs": [],
   "source": [
    "# end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
