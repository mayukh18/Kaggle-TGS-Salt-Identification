{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "08b409e8fd4ae201e80308cb7c1290937d743a5d"
   },
   "source": [
    "# ResUnet\n",
    "\n",
    "1. Unet with 3 residual blocks at each level\n",
    "2. Input layer 101*101, 16 channels\n",
    "3. Middle layer 6*6, 512 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# import cv2\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "\n",
    "from tqdm import tqdm_notebook #, tnrange\n",
    "#from itertools import chain\n",
    "from skimage.io import imread, imshow #, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "from keras.models import Model, load_model, save_model\n",
    "from keras.layers import Input,Dropout,BatchNormalization,Activation,Add,SpatialDropout2D,SpatialDropout3D\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img#,save_img\n",
    "\n",
    "import time\n",
    "t_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c36d16775556c3e358edefab5710dc1541ccfc4"
   },
   "outputs": [],
   "source": [
    "version = 5\n",
    "basic_name = f'Unet_resnet_v{version}'\n",
    "save_model_name = 'resunet4.model'\n",
    "submission_file = basic_name + '.csv'\n",
    "\n",
    "print(save_model_name)\n",
    "print(submission_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "63c469280793719bf311d51e6ba2cdaea157d175"
   },
   "outputs": [],
   "source": [
    "img_size_ori = 101\n",
    "img_size_target = 101\n",
    "\n",
    "def upsample(img):# not used\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    \n",
    "def downsample(img):# not used\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a64babef03b9a0dbc94387a1dad54971c3e028d"
   },
   "outputs": [],
   "source": [
    "# Loading of training/testing ids and depths\n",
    "train_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]\n",
    "\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80c3768717007fb5f087d3e01619f1a9f9a3beac"
   },
   "outputs": [],
   "source": [
    "train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f55103f7daad6f03ec874c643077fe686c31bee"
   },
   "outputs": [],
   "source": [
    "train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "010066dd50ef4fdfa7dabe2c946fd7491f9556fd"
   },
   "outputs": [],
   "source": [
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)\n",
    "\n",
    "def cov_to_class(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "        \n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ad5ac1576277fc54d768933c36efd1f9ff01acd"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "sns.distplot(train_df.coverage, kde=False, ax=axs[0])\n",
    "sns.distplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\n",
    "plt.suptitle(\"Salt coverage\")\n",
    "axs[0].set_xlabel(\"Coverage\")\n",
    "axs[1].set_xlabel(\"Coverage class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f34c4263989a3d95af1e5922c1b7d2126655610"
   },
   "outputs": [],
   "source": [
    "#Plotting the depth distributionsÂ¶\n",
    "\n",
    "sns.distplot(train_df.z, label=\"Train\")\n",
    "sns.distplot(test_df.z, label=\"Test\")\n",
    "plt.legend()\n",
    "plt.title(\"Depth distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9bbb0110ad07be52129c9f9194ad10a1abdc89a0"
   },
   "outputs": [],
   "source": [
    "ids_train = []\n",
    "ids_valid = []\n",
    "skf = StratifiedKFold(n_splits=5, random_state=4701)\n",
    "i = 0\n",
    "for indices in skf.split(X=train_df.index.values,y=train_df.coverage_class):\n",
    "    if i==4:\n",
    "        ids_train = indices[0]\n",
    "        ids_valid = indices[1]\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "x_train = np.array(train_df.images.tolist()).reshape(-1, img_size_target, img_size_target, 1)[ids_train]\n",
    "x_valid = np.array(train_df.images.tolist()).reshape(-1, img_size_target, img_size_target, 1)[ids_valid]\n",
    "y_train = np.array(train_df.masks.tolist()).reshape(-1, img_size_target, img_size_target, 1)[ids_train]\n",
    "y_valid = np.array(train_df.masks.tolist()).reshape(-1, img_size_target, img_size_target, 1)[ids_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7fb577cdf27f365d4a912728c2a7654d0e60fac8"
   },
   "outputs": [],
   "source": [
    "def BatchActivate(x):\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n",
    "    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n",
    "    if activation == True:\n",
    "        x = BatchActivate(x)\n",
    "    return x\n",
    "\n",
    "def residual_block(blockInput, num_filters=16, batch_activate = False):\n",
    "    x = BatchActivate(blockInput)\n",
    "    x = convolution_block(x, num_filters, (3,3) )\n",
    "    x = convolution_block(x, num_filters, (3,3), activation=False)\n",
    "    x = Add()([x, blockInput])\n",
    "    if batch_activate:\n",
    "        x = BatchActivate(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "02967d71ee7f936254ab54acf2aa7c2e038a2b21"
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model(input_layer, start_neurons, DropoutRatio = 0.4):\n",
    "    # 101 -> 50\n",
    "    conv1a = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(input_layer)\n",
    "    conv1 = residual_block(conv1a,start_neurons * 1)\n",
    "    conv1 = residual_block(conv1,start_neurons * 1)\n",
    "    conv1 = Add()([conv1, conv1a])\n",
    "    conv1 = residual_block(conv1,start_neurons * 1, True)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    pool1 = Dropout(DropoutRatio/2)(pool1)\n",
    "\n",
    "    # 50 -> 25\n",
    "    conv2a = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(pool1)\n",
    "    conv2 = residual_block(conv2a,start_neurons * 2)\n",
    "    conv2 = residual_block(conv2,start_neurons * 2)\n",
    "    conv2 = Add()([conv2, conv2a])\n",
    "    conv2 = residual_block(conv2,start_neurons * 2, True)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    pool2 = Dropout(DropoutRatio)(pool2)\n",
    "\n",
    "    # 25 -> 12\n",
    "    conv3a = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(pool2)\n",
    "    conv3 = residual_block(conv3a,start_neurons * 4)\n",
    "    conv3 = residual_block(conv3,start_neurons * 4)\n",
    "    conv3 = Add()([conv3, conv3a])\n",
    "    conv3 = residual_block(conv3,start_neurons * 4, True)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    pool3 = Dropout(DropoutRatio)(pool3)\n",
    "\n",
    "    # 12 -> 6\n",
    "    conv4a = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(pool3)\n",
    "    conv4 = residual_block(conv4a,start_neurons * 8)\n",
    "    conv4 = residual_block(conv4,start_neurons * 8)\n",
    "    conv4 = Add()([conv4, conv4a])\n",
    "    conv4 = residual_block(conv4,start_neurons * 8, True)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(DropoutRatio)(pool4)\n",
    "    \n",
    "    # 12 -> 6\n",
    "    conv4aa = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(pool4)\n",
    "    conv4a = residual_block(conv4aa,start_neurons * 16)\n",
    "    conv4a = residual_block(conv4a,start_neurons * 16)\n",
    "    conv4a = Add()([conv4a, conv4aa])\n",
    "    conv4a = residual_block(conv4a,start_neurons * 16, True)\n",
    "    conv4a = BatchNormalization()(conv4a)\n",
    "    pool4a = MaxPooling2D((2, 2))(conv4a)\n",
    "    pool4a = Dropout(DropoutRatio)(pool4a)\n",
    "\n",
    "    # Middle\n",
    "    convma = Conv2D(start_neurons * 32, (3, 3), activation=None, padding=\"same\")(pool4a)\n",
    "    convm = residual_block(convma,start_neurons * 32)\n",
    "    convm = residual_block(convm,start_neurons * 32)\n",
    "    convm = Add()([convm, convma])\n",
    "    convm = residual_block(convm,start_neurons * 32, True)\n",
    "    convm = BatchNormalization()(convm)\n",
    "    \n",
    "    # 6 -> 12\n",
    "    deconv4a = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv4a = concatenate([deconv4a, conv4a])\n",
    "    uconv4a = Dropout(DropoutRatio)(uconv4a)\n",
    "    \n",
    "    uconv4aa = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(uconv4a)\n",
    "    uconv4a = residual_block(uconv4aa,start_neurons * 16)\n",
    "    uconv4a = residual_block(uconv4a,start_neurons * 16)\n",
    "    uconv4a = Add()([uconv4a, uconv4aa])\n",
    "    uconv4a = residual_block(uconv4a,start_neurons * 16, True)\n",
    "    uconv4a = BatchNormalization()(uconv4a)\n",
    "    \n",
    "    # 6 -> 12\n",
    "    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(uconv4a)\n",
    "    uconv4 = concatenate([deconv4, conv4])\n",
    "    uconv4 = Dropout(DropoutRatio)(uconv4)\n",
    "    \n",
    "    uconv4a = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv4)\n",
    "    uconv4 = residual_block(uconv4a,start_neurons * 8)\n",
    "    uconv4 = residual_block(uconv4,start_neurons * 8)\n",
    "    uconv4 = Add()([uconv4, uconv4a])\n",
    "    uconv4 = residual_block(uconv4,start_neurons * 8, True)\n",
    "    uconv4 = BatchNormalization()(uconv4)\n",
    "    \n",
    "    # 12 -> 25\n",
    "    #deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n",
    "    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"valid\")(uconv4)\n",
    "    uconv3 = concatenate([deconv3, conv3])    \n",
    "    uconv3 = Dropout(DropoutRatio)(uconv3)\n",
    "    \n",
    "    uconv3a = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv3)\n",
    "    uconv3 = residual_block(uconv3a,start_neurons * 4)\n",
    "    uconv3 = residual_block(uconv3,start_neurons * 4)\n",
    "    uconv3 = Add()([uconv3, uconv3a])\n",
    "    uconv3 = residual_block(uconv3,start_neurons * 4, True)\n",
    "    uconv3 = BatchNormalization()(uconv3)\n",
    "\n",
    "    # 25 -> 50\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "    uconv2 = Dropout(DropoutRatio)(uconv2)\n",
    "    \n",
    "    uconv2a = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv2)\n",
    "    uconv2 = residual_block(uconv2a,start_neurons * 2)\n",
    "    uconv2 = residual_block(uconv2,start_neurons * 2)\n",
    "    uconv2 = Add()([uconv2, uconv2a])\n",
    "    uconv2 = residual_block(uconv2,start_neurons * 2, True)\n",
    "    uconv2 = BatchNormalization()(uconv2)\n",
    "    \n",
    "    # 50 -> 101\n",
    "    #deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"valid\")(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    uconv1 = Dropout(DropoutRatio)(uconv1)\n",
    "    \n",
    "    uconv1a = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv1)\n",
    "    uconv1 = residual_block(uconv1a,start_neurons * 1)\n",
    "    uconv1 = residual_block(uconv1,start_neurons * 1)\n",
    "    uconv1 = Add()([uconv1, uconv1a])\n",
    "    uconv1 = residual_block(uconv1,start_neurons * 1, True)\n",
    "    uconv1 = BatchNormalization()(uconv1)\n",
    "    \n",
    "    output_layer_noActi = Conv2D(1, (1,1), padding=\"same\", activation=None)(uconv1)\n",
    "    output_layer =  Activation('sigmoid')(output_layer_noActi)\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2bd5e479d3aa211bcb5fe32ce9c4d71cd012eefa"
   },
   "outputs": [],
   "source": [
    "def get_iou_vector(A, B):\n",
    "    batch_size = A.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        t, p = A[batch]>0, B[batch]>0\n",
    "        \n",
    "        intersection = np.logical_and(t, p)\n",
    "        union = np.logical_or(t, p)\n",
    "        iou = (np.sum(intersection > 0) + 1e-10 )/ (np.sum(union > 0) + 1e-10)\n",
    "        thresholds = np.arange(0.5, 1, 0.05)\n",
    "        s = []\n",
    "        for thresh in thresholds:\n",
    "            s.append(iou > thresh)\n",
    "        metric.append(np.mean(s))\n",
    "\n",
    "    return np.mean(metric)\n",
    "\n",
    "def my_iou_metric(label, pred):\n",
    "    return tf.py_func(get_iou_vector, [label, pred>0.5], tf.float64)\n",
    "\n",
    "def my_iou_metric_2(label, pred):\n",
    "    return tf.py_func(get_iou_vector, [label, pred >0], tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e25caa2102813289cbd74796b612fbbfaaff8154"
   },
   "outputs": [],
   "source": [
    "# code download from: https://github.com/bermanmaxim/LovaszSoftmax\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    gts = tf.reduce_sum(gt_sorted)\n",
    "    intersection = gts - tf.cumsum(gt_sorted)\n",
    "    union = gts + tf.cumsum(1. - gt_sorted)\n",
    "    jaccard = 1. - intersection / union\n",
    "    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "# --------------------------- BINARY LOSSES ---------------------------\n",
    "\n",
    "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        def treat_image(log_lab):\n",
    "            log, lab = log_lab\n",
    "            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n",
    "            log, lab = flatten_binary_scores(log, lab, ignore)\n",
    "            return lovasz_hinge_flat(log, lab)\n",
    "        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n",
    "        loss = tf.reduce_mean(losses)\n",
    "    else:\n",
    "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss():\n",
    "        labelsf = tf.cast(labels, logits.dtype)\n",
    "        signs = 2. * labelsf - 1.\n",
    "        errors = 1. - logits * tf.stop_gradient(signs)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n",
    "        gt_sorted = tf.gather(labelsf, perm)\n",
    "        grad = lovasz_grad(gt_sorted)\n",
    "        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
    "        return loss\n",
    "\n",
    "    # deal with the void prediction case (only void pixels)\n",
    "    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n",
    "                   lambda: tf.reduce_sum(logits) * 0.,\n",
    "                   compute_loss,\n",
    "                   strict=True,\n",
    "                   name=\"loss\"\n",
    "                   )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def flatten_binary_scores(scores, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch (binary case)\n",
    "    Remove labels equal to 'ignore'\n",
    "    \"\"\"\n",
    "    scores = tf.reshape(scores, (-1,))\n",
    "    labels = tf.reshape(labels, (-1,))\n",
    "    if ignore is None:\n",
    "        return scores, labels\n",
    "    valid = tf.not_equal(labels, ignore)\n",
    "    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n",
    "    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n",
    "    return vscores, vlabels\n",
    "\n",
    "def lovasz_loss(y_true, y_pred):\n",
    "    y_true, y_pred = K.cast(K.squeeze(y_true, -1), 'int32'), K.cast(K.squeeze(y_pred, -1), 'float32')\n",
    "    #logits = K.log(y_pred / (1. - y_pred))\n",
    "    logits = y_pred #Jiaxin\n",
    "    loss = lovasz_hinge(logits, y_true, per_image = True, ignore = None)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "072ab621d38cc93d26998f391357cb6efc791600"
   },
   "outputs": [],
   "source": [
    "#Data augmentation\n",
    "x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n",
    "y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)\n",
    "print(x_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "30622932f68888e895a9b8cac91810a1bb3c5e75"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "input_layer = Input((img_size_target, img_size_target, 1))\n",
    "output_layer = build_model(input_layer, 16,0.5)\n",
    "\n",
    "model1 = Model(input_layer, output_layer)\n",
    "\n",
    "c = optimizers.adam(lr = 0.0105)\n",
    "model1.compile(loss=\"binary_crossentropy\", optimizer=c, metrics=[my_iou_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "41699081be465c14e193ffad4fd00bd56840f156"
   },
   "outputs": [],
   "source": [
    "#early_stopping = EarlyStopping(monitor='my_iou_metric', mode = 'max',patience=10, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(save_model_name,monitor='my_iou_metric', \n",
    "                                   mode = 'max', save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='my_iou_metric', mode = 'max',factor=0.5, patience=5, min_lr=0.0001, verbose=1)\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "history = model1.fit(x_train, y_train,\n",
    "                    validation_data=[x_valid, y_valid], \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[ model_checkpoint,reduce_lr], \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "513a3f8490ae5cbc61651458750ba369f99f6f62",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1 = load_model(save_model_name,custom_objects={'my_iou_metric': my_iou_metric})\n",
    "# remove layter activation layer and use losvasz loss\n",
    "input_x = model1.layers[0].input\n",
    "\n",
    "output_layer = model1.layers[-1].input\n",
    "model = Model(input_x, output_layer)\n",
    "c = optimizers.adam(lr = 0.0105)\n",
    "\n",
    "# lovasz_loss need input range (-âï¼+â), so cancel the last \"sigmoid\" activation  \n",
    "# Then the default threshod for pixel prediction is 0 instead of 0.5, as in my_iou_metric_2.\n",
    "model.compile(loss=lovasz_loss, optimizer=c, metrics=[my_iou_metric_2])\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc68a1d9f9ec3837a2988bd7fe05d1fffdbc381d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#early_stopping = EarlyStopping(monitor='val_my_iou_metric_2', mode = 'max',patience=20, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(save_model_name,monitor='val_my_iou_metric_2', \n",
    "                                   mode = 'max', save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric_2', mode = 'max',factor=0.5, patience=5, min_lr=0.0001, verbose=1)\n",
    "epochs = 45\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=[x_valid, y_valid], \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[ model_checkpoint,reduce_lr], \n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "347c6567c95430f28ec51b94f42603e37a4056db",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, (ax_loss, ax_score) = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "ax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax_loss.legend()\n",
    "ax_score.plot(history.epoch, history.history[\"my_iou_metric_2\"], label=\"Train score\")\n",
    "ax_score.plot(history.epoch, history.history[\"val_my_iou_metric_2\"], label=\"Validation score\")\n",
    "ax_score.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a081f8f61a713457c8c8f3979a78f75541875456",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_model(model, save_model_name)\n",
    "model = load_model(save_model_name,custom_objects={'my_iou_metric_2': my_iou_metric_2,\n",
    "                                                   'lovasz_loss': lovasz_loss})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f68651e1ce6ad9a461c8f1a25a1250fd489adfed",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_result(model,x_test,img_size_target): # predict both orginal and reflect x\n",
    "    x_test_reflect =  np.array([np.fliplr(x) for x in x_test])\n",
    "    preds_test = model.predict(x_test).reshape(-1, img_size_target, img_size_target)\n",
    "    preds_test2_refect = model.predict(x_test_reflect).reshape(-1, img_size_target, img_size_target)\n",
    "    preds_test += np.array([ np.fliplr(x) for x in preds_test2_refect] )\n",
    "    return preds_test/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b7a061700364ea17735f953d6bd3c835bc3dc630",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_valid = predict_result(model,x_valid,img_size_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5a9b1a900a8a4d031978c3b5ef2e739b5661de73",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Score the model and do a threshold optimization by the best IoU.\n",
    "\n",
    "# src: https://www.kaggle.com/aglotero/another-iou-metric\n",
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = y_true_in\n",
    "    y_pred = y_pred_in\n",
    "\n",
    "\n",
    "    true_objects = 2\n",
    "    pred_objects = 2\n",
    "\n",
    "    #  if all zeros, original code  generate wrong  bins [-0.5 0 0.5],\n",
    "    temp1 = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=([0,0.5,1], [0,0.5, 1]))\n",
    "#     temp1 = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))\n",
    "    #print(temp1)\n",
    "    intersection = temp1[0]\n",
    "    #print(\"temp2 = \",temp1[1])\n",
    "    #print(intersection.shape)\n",
    "   # print(intersection)\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    #print(np.histogram(labels, bins = true_objects))\n",
    "    area_true = np.histogram(labels,bins=[0,0.5,1])[0]\n",
    "    #print(\"area_true = \",area_true)\n",
    "    area_pred = np.histogram(y_pred, bins=[0,0.5,1])[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "  \n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    intersection[intersection == 0] = 1e-9\n",
    "    \n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "        metric.append(value)\n",
    "    return np.mean(metric)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91b1be75d7e8ff74db956c8c846c47ed7576ecb2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Scoring for last model, choose threshold by validation data \n",
    "thresholds_ori = np.linspace(0.3, 0.7, 31)\n",
    "# Reverse sigmoid function: Use code below because the  sigmoid activation was removed\n",
    "thresholds = np.log(thresholds_ori/(1-thresholds_ori)) \n",
    "\n",
    "# ious = np.array([get_iou_vector(y_valid, preds_valid > threshold) for threshold in tqdm_notebook(thresholds)])\n",
    "# print(ious)\n",
    "ious = np.array([iou_metric_batch(y_valid, preds_valid > threshold) for threshold in tqdm_notebook(thresholds)])\n",
    "print(ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1beb285910aa6532d7a69a70afa6a9671e6d347d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instead of using default 0 as threshold, use validation data to find the best threshold.\n",
    "threshold_best_index = np.argmax(ious) \n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]\n",
    "\n",
    "plt.plot(thresholds, ious)\n",
    "plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c4852ecf2c504a4ce30e975f5e36b534c2111e6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "used for converting the decoded image to rle mask\n",
    "Fast compared to previous one\n",
    "\"\"\"\n",
    "def rle_encode(im):\n",
    "    '''\n",
    "    im: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = im.flatten(order = 'F')\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "21bb50645b1c67d907e42c262d92f975668fccac",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.array([(np.array(load_img(\"../input/test/images/{}.png\".format(idx), grayscale = True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce8185fa64aafbe42ebde98aba984b4f22b0050e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_test = predict_result(model,x_test,img_size_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7ccac81a492b9caaff4a25401165e145cf2c6f8e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "pred_dict = {idx: rle_encode(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}\n",
    "t2 = time.time()\n",
    "\n",
    "print(f\"Usedtime = {t2-t1} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "770d7d596656f4f1ad17a6063ad662ac80e11b24",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub.to_csv(submission_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c89c406884ee54bee2c57aff51b116c157553ae9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_finish = time.time()\n",
    "print(f\"Kernel run time = {(t_finish-t_start)/3600} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a3009187c164635fbe163bd8ce406c2e309b1e5f",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
